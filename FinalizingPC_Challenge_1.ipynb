{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "iHGYx97cJqaE",
        "FlMu4vgjXZAF",
        "sgA4WwuXXdm1",
        "RpyBtiheW-in",
        "t4eaFcePJ_8r",
        "_drFXRZgau3u"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Challenge 1**"
      ],
      "metadata": {
        "id": "xvdtBiPrGllf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a **batched arbitrarily-size matrix multiplication** kernel.\n",
        "\n",
        "Let dimensions be identical for all matrix multiplications in the batch.<br>\n",
        "These being $m, k, n \\in \\mathbb{N}$.<br>\n",
        "While $batch \\in \\mathbb{N}$ is the batch size.\n",
        "\n",
        "You are provided as input the following matrices:<br>\n",
        "$N_0, N_1, N_2, ... N_{batch - 1} \\in \\mathbb{M}^{k \\times n}$<br>\n",
        "$M \\in \\mathbb{M}^{m \\times k}$\n",
        "\n",
        "You need to compute:<br>\n",
        "$P_0, P_1, P_2, ... P_{batch - 1} \\in \\mathbb{M}^{m \\times n}$\n",
        "\n",
        "Where $P_i = M \\otimes N_i$ for each $i \\in \\{0, ..., batch - 1\\}$.\n",
        "\n",
        "---\n",
        "\n",
        "A baseline reference implementation is given. Implement your version to replace it. Rely on the provided host-side function to check the correctness of results.\n",
        "\n",
        "To get a general performance metric rely on the profiler, specifically look for the `cuda_gpu_kern_sum` and try to minimize the `Total Time (ns)` of your kernel. Meanwhile, you may also want to improve `cuda_gpu_mem_time_sum`.\n",
        "\n",
        "Step one is beating the reference implementation, that should be easy, then you can use all tricks in the book to push it further.\n",
        "Anything goes, but if you use \"exotic\" tricks we want an explanation.\n",
        "In fact, submitting your work, be sure to fill out the [report](#report) with brief insights of what you did."
      ],
      "metadata": {
        "id": "xcDOQnQMGwLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "General rules and advice:\n",
        "- groups are of 3 members at most, that should, for as much as possible, equally contribute to the project.\n",
        "- you automagically get ~3 points in the 1st part of the exam, taking the form of 1-2 questions that you will be allowed to skip.\n",
        "- deadline is 1 week (as of this writing, you will need to submit your work before october 23 at 23:59).\n",
        "- submissions are to be made on WeBeep, where you need to upload a downloaded copy of this notebook (.ipynb file); for groups of multiple people, it's enough for one member to submit the file in the assignment, other members shall simply write their group's name in their submission, we will then infer groups from what you write in the report section.\n",
        "- your code needs to work here on Colab with the T4 runtime.\n",
        "- do not alter code sections delimited by \"===\"s in the final submission.\n",
        "- we will change around matrix sizes arbitrarily while evaluating your work, so make sure to cover all edge cases and take care that your code is scalable (e.g. execution time grows as expected when doubling all dimensions).\n",
        "- you can get the maximum grade just by using what was discussed during lectures or is present in the glossary shown during exercise sessions; still, if you wanna have \"more fun\" this guide is your best friend https://docs.nvidia.com/cuda/cuda-c-best-practices-guide.\n",
        "- a piece of code that works is better than a supposedly faster piece of code that doesn't, so don't go overboard, but be ambitious.\n",
        "- use LLMs (ChatGPT and friends) responsibly; the purpose of this challenge is for you to get your hands dirty and build up confidence in writing parallel code through trial and error. Having an LLM write your code may get you the challenge's points (unless it's so blatant that we notice), but won't lead you to learn anything and the next time you see some parallel code your mind goes blank. If you wack your head at the problem instead, and solve it, the solution will stick in the back of your mind for a long time. Similarly, if despite pushing yourself you can't find \"that damn bug\", then asking an LLM is fine, so long as you tried first by yourself and just say \"ahhhhhh, so that what it was!\" upon having the LLM help you out. Long story short, AI is fine so long as it's a tool you **learn from** and **not** one you **blindly lean on**.\n",
        "\n",
        "If you need help or anything, please drop us an email:\n",
        "- Dr. M. Ronzani: marco.ronzani@polimi.it\n",
        "- Prof. F. Ferrandi: fabrizio.ferrandi@polimi.it"
      ],
      "metadata": {
        "id": "t0olg8POKfsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Colab Setup**"
      ],
      "metadata": {
        "id": "iHGYx97cJqaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
        "!apt update\n",
        "!apt install ./nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
        "!apt --fix-broken install"
      ],
      "metadata": {
        "id": "MWrw0NgzGlMq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ITAYKD7MGcmH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e4416899-7faa-4e43-8560-c2f5fdd490b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/home/cuda’: File exists\n",
            "/home/cuda\n"
          ]
        }
      ],
      "source": [
        "!mkdir /home/cuda\n",
        "%cd /home/cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Code**"
      ],
      "metadata": {
        "id": "F9rbKG58Jyw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Baseline**: Naive batched\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "* naive reference CUDA kernel implementation. Each thread computes one output element using direct global memory access without optimization. Used as baseline for performance comparison\n",
        "\n"
      ],
      "metadata": {
        "id": "FlMu4vgjXZAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "// this is the reference implementation\n",
        "// you can change this to your heart's contempt\n",
        "__global__ void batchedMatMul(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int b = blockIdx.z;  // New: batch index from grid.z\n",
        "\n",
        "  if (row < m && col < n && b < batch) {  // Guard b too\n",
        "    float value = 0.0f;\n",
        "    for (int i = 0; i < k; i++) {\n",
        "      float a = M[row * k + i];\n",
        "      float c = N[b * (k * n) + i * n + col];\n",
        "      value += a * c;\n",
        "    }\n",
        "    P[b * (m * n) + row * n + col] = value;\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "\n",
        "  float* M = (float*)malloc(sizeM * sizeof(float));\n",
        "  float* N = (float*)malloc(sizeN * sizeof(float));\n",
        "  float* P = (float*)malloc(sizeP * sizeof(float));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything\n",
        "  float *M_d;\n",
        "  float *N_d;\n",
        "  float *P_d;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&M_d, sizeM * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(M_d, M, sizeM * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(P_d, P, sizeP * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "  dim3 blockSize(16, 16);\n",
        "  dim3 numBlocks((n + blockSize.x - 1) / blockSize.x, (m + blockSize.y - 1) / blockSize.y, batch);\n",
        "  batchedMatMul<<<numBlocks, blockSize>>>(M_d, N_d, P_d, m, k, n, batch);\n",
        "\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything, e.g. add some logging\n",
        "  gpuErrchk(cudaFree(M_d));\n",
        "  gpuErrchk(cudaFree(N_d));\n",
        "  gpuErrchk(cudaFree(P_d));\n",
        "\n",
        "  free(M);\n",
        "  free(N);\n",
        "  free(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bLGAehnPXHaJ",
        "outputId": "b297b85f-0d63-4b34-938e-636aa03d6fd9"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing bmatmul.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv bmatmul.cpp bmatmul.cu\n",
        "!nvcc -arch=sm_75 bmatmul.cu -o bmatmul"
      ],
      "metadata": {
        "id": "vUnJ96hoXPq_"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./bmatmul 256 512 384 16 119"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "aXkXGj6PXQ6G",
        "outputId": "2cc552e9-2aba-4239-afe8-99ce12312ee0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running batched matmul with m=256, k=512, n=384, batch=16, seed=119\n",
            "Checking results on CPU...\n",
            "All results matched, success!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile --stats=true ./bmatmul 256 512 384 16 119"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UHTmHVpKXR4-",
        "outputId": "698f3e32-1696-42b9-b7b9-099093d57fff"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running batched matmul with m=256, k=512, n=384, batch=16, seed=119\n",
            "Checking results on CPU...\n",
            "All results matched, success!Generating '/tmp/nsys-report-07dc.qdstrm'\n",
            "[1/8] [========================100%] report7.nsys-rep\n",
            "[2/8] [========================100%] report7.sqlite\n",
            "[3/8] Executing 'nvtx_sum' stats report\n",
            "SKIPPED: /home/cuda/report7.sqlite does not contain NV Tools Extension (NVTX) data.\n",
            "[4/8] Executing 'osrt_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)   Max (ns)    StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  -------------  --------  -----------  ------------  ----------------------\n",
            "     98.9    4,813,581,591         57  84,448,799.8  100,151,806.0    59,832  303,696,667  48,186,118.4  poll                  \n",
            "      1.1       51,270,624        543      94,421.0       14,568.0       626   18,504,377     804,278.7  ioctl                 \n",
            "      0.0        2,112,774         31      68,154.0       13,053.0     7,759    1,277,406     226,093.5  mmap64                \n",
            "      0.0          606,634         10      60,663.4       62,477.5    14,464      133,587      33,467.1  sem_timedwait         \n",
            "      0.0          539,994          1     539,994.0      539,994.0   539,994      539,994           0.0  pthread_cond_wait     \n",
            "      0.0          491,392         49      10,028.4        8,378.0     2,011       34,187       6,092.7  open64                \n",
            "      0.0          252,107         40       6,302.7        3,253.5     1,565       39,030       7,466.5  fopen                 \n",
            "      0.0          225,780         15      15,052.0        9,787.0     2,565       75,690      18,660.7  mmap                  \n",
            "      0.0          122,799          2      61,399.5       61,399.5    47,473       75,326      19,695.0  pthread_create        \n",
            "      0.0           67,368         12       5,614.0        5,709.0     2,332        8,903       2,030.6  write                 \n",
            "      0.0           52,680         33       1,596.4        1,239.0       706        6,198       1,181.7  fclose                \n",
            "      0.0           41,283         20       2,064.2           54.0        53       40,139       8,961.9  fgets                 \n",
            "      0.0           36,959          6       6,159.8        6,215.5     3,982        8,598       1,677.2  munmap                \n",
            "      0.0           36,696          2      18,348.0       18,348.0     7,314       29,382      15,604.4  socket                \n",
            "      0.0           35,348         64         552.3          572.5       190        1,171         197.1  fcntl                 \n",
            "      0.0           31,812          6       5,302.0        5,087.0     1,798        9,198       2,888.5  open                  \n",
            "      0.0           23,463         15       1,564.2        1,328.0       666        3,328         769.2  read                  \n",
            "      0.0           18,055          3       6,018.3        6,478.0     3,071        8,506       2,746.5  pipe2                 \n",
            "      0.0           10,716          1      10,716.0       10,716.0    10,716       10,716           0.0  connect               \n",
            "      0.0            7,792          2       3,896.0        3,896.0     2,356        5,436       2,177.9  pthread_cond_broadcast\n",
            "      0.0            5,869          2       2,934.5        2,934.5     2,775        3,094         225.6  fwrite                \n",
            "      0.0            3,675          8         459.4          443.5       307          664         121.2  dup                   \n",
            "      0.0            1,450          1       1,450.0        1,450.0     1,450        1,450           0.0  bind                  \n",
            "      0.0              906          1         906.0          906.0       906          906           0.0  listen                \n",
            "\n",
            "[5/8] Executing 'cuda_api_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)     Med (ns)    Min (ns)    Max (ns)   StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  -----------  ---------  ----------  ------------  ----------------------\n",
            "     84.5       76,576,294          3  25,525,431.3     85,024.0     72,058  76,419,212  44,075,307.4  cudaMalloc            \n",
            "      7.7        7,011,476          1   7,011,476.0  7,011,476.0  7,011,476   7,011,476           0.0  cudaDeviceSynchronize \n",
            "      6.5        5,894,758          4   1,473,689.5  1,456,231.0    179,630   2,802,666   1,072,064.9  cudaMemcpy            \n",
            "      1.0          948,678          3     316,226.0    278,223.0    129,053     541,402     208,784.8  cudaFree              \n",
            "      0.2          150,088          1     150,088.0    150,088.0    150,088     150,088           0.0  cudaLaunchKernel      \n",
            "      0.0            1,764          1       1,764.0      1,764.0      1,764       1,764           0.0  cuModuleGetLoadingMode\n",
            "\n",
            "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                              Name                            \n",
            " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ------------------------------------------------------------\n",
            "    100.0        7,006,842          1  7,006,842.0  7,006,842.0  7,006,842  7,006,842          0.0  batchedMatMul(float *, float *, float *, int, int, int, int)\n",
            "\n",
            "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Count   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)      Operation     \n",
            " --------  ---------------  -----  -----------  -----------  ---------  ---------  -----------  ------------------\n",
            "     78.6        3,861,989      3  1,287,329.7  1,218,915.0     46,239  2,596,835  1,276,673.6  [CUDA memcpy HtoD]\n",
            "     21.4        1,048,967      1  1,048,967.0  1,048,967.0  1,048,967  1,048,967          0.0  [CUDA memcpy DtoH]\n",
            "\n",
            "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
            "\n",
            " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n",
            " ----------  -----  --------  --------  --------  --------  -----------  ------------------\n",
            "     19.399      3     6.466     6.291     0.524    12.583        6.031  [CUDA memcpy HtoD]\n",
            "      6.291      1     6.291     6.291     6.291     6.291        0.000  [CUDA memcpy DtoH]\n",
            "\n",
            "Generated:\n",
            "    /home/cuda/report7.nsys-rep\n",
            "    /home/cuda/report7.sqlite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FIRST TRIAL:** Loop Unrolling with pragma unroll\n",
        "*Attempt to help the compiler optimize the inner loop by adding #pragma unroll. Expected small gain due to still using global memory repeatedly*"
      ],
      "metadata": {
        "id": "sgA4WwuXXdm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "// this is the reference implementation\n",
        "// you can change this to your heart's contempt\n",
        "__global__\n",
        "void batchedMatMul(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int b = blockIdx.z;  // New: batch index from grid.z\n",
        "\n",
        "  if (row < m && col < n && b < batch) {  // Guard b too\n",
        "    float value = 0.0f;\n",
        "    #pragma unroll\n",
        "    for (int i = 0; i < k; i++) {\n",
        "      float a = M[row * k + i];\n",
        "      float c = N[b * (k * n) + i * n + col];\n",
        "      value += a * c;\n",
        "    }\n",
        "    P[b * (m * n) + row * n + col] = value;\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "\n",
        "  float* M = (float*)malloc(sizeM * sizeof(float));\n",
        "  float* N = (float*)malloc(sizeN * sizeof(float));\n",
        "  float* P = (float*)malloc(sizeP * sizeof(float));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything\n",
        "  float *M_d;\n",
        "  float *N_d;\n",
        "  float *P_d;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&M_d, sizeM * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(M_d, M, sizeM * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(P_d, P, sizeP * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "  dim3 blockSize(16, 16);\n",
        "  dim3 numBlocks((n + blockSize.x - 1) / blockSize.x, (m + blockSize.y - 1) / blockSize.y, batch);\n",
        "  batchedMatMul<<<numBlocks, blockSize>>>(M_d, N_d, P_d, m, k, n, batch);\n",
        "\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything, e.g. add some logging\n",
        "  gpuErrchk(cudaFree(M_d));\n",
        "  gpuErrchk(cudaFree(N_d));\n",
        "  gpuErrchk(cudaFree(P_d));\n",
        "\n",
        "  free(M);\n",
        "  free(N);\n",
        "  free(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yv7BknpqXhL1",
        "outputId": "056f6f0a-6a25-4550-d1ca-6aec1c9e6f18"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing bmatmul.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv bmatmul.cpp bmatmul.cu\n",
        "!nvcc -arch=sm_75 bmatmul.cu -o bmatmul"
      ],
      "metadata": {
        "id": "WJCyf7p3Xj9m"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./bmatmul 256 512 384 16 119"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XaG0isWiXlMO",
        "outputId": "203093aa-d8cf-4891-b2ab-97013145af92"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running batched matmul with m=256, k=512, n=384, batch=16, seed=119\n",
            "Checking results on CPU...\n",
            "All results matched, success!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile --stats=true ./bmatmul 256 512 384 16 119"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xke2_BgFXnGV",
        "outputId": "943d5514-b70c-406a-cb1c-ef38c7fac079"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running batched matmul with m=256, k=512, n=384, batch=16, seed=119\n",
            "Checking results on CPU...\n",
            "All results matched, success!Generating '/tmp/nsys-report-cb78.qdstrm'\n",
            "[1/8] [========================100%] report8.nsys-rep\n",
            "[2/8] [========================100%] report8.sqlite\n",
            "[3/8] Executing 'nvtx_sum' stats report\n",
            "SKIPPED: /home/cuda/report8.sqlite does not contain NV Tools Extension (NVTX) data.\n",
            "[4/8] Executing 'osrt_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)   Max (ns)    StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  -------------  --------  -----------  ------------  ----------------------\n",
            "     98.9    4,706,074,578         56  84,037,046.0  100,147,740.0     2,630  299,278,676  48,285,665.2  poll                  \n",
            "      1.0       48,666,640        543      89,625.5       13,267.0       401   17,960,153     779,810.0  ioctl                 \n",
            "      0.0        2,068,235         31      66,717.3       13,101.0     7,983    1,319,878     233,490.8  mmap64                \n",
            "      0.0          575,949         10      57,594.9       51,317.0    25,813      131,956      28,197.8  sem_timedwait         \n",
            "      0.0          484,049          1     484,049.0      484,049.0   484,049      484,049           0.0  pthread_cond_wait     \n",
            "      0.0          409,954         49       8,366.4        7,770.0     2,149       26,488       3,626.9  open64                \n",
            "      0.0          240,801         40       6,020.0        3,664.5     1,482       39,013       7,350.8  fopen                 \n",
            "      0.0          186,197         15      12,413.1        9,792.0     1,756       56,147      13,731.8  mmap                  \n",
            "      0.0           96,723          2      48,361.5       48,361.5    41,636       55,087       9,511.3  pthread_create        \n",
            "      0.0           57,820         12       4,818.3        4,914.5     1,575        7,380       1,636.1  write                 \n",
            "      0.0           53,905         33       1,633.5        1,189.0       709        5,769       1,196.1  fclose                \n",
            "      0.0           41,649          6       6,941.5        4,747.0     1,764       19,762       6,601.0  open                  \n",
            "      0.0           38,060         20       1,903.0           48.0        48       37,032       8,268.5  fgets                 \n",
            "      0.0           32,341         64         505.3          541.0       164        1,083         178.5  fcntl                 \n",
            "      0.0           29,966          6       4,994.3        5,384.0     3,147        6,580       1,392.3  munmap                \n",
            "      0.0           22,845         15       1,523.0        1,389.0       903        3,165         661.5  read                  \n",
            "      0.0           21,249          2      10,624.5       10,624.5     7,623       13,626       4,244.8  socket                \n",
            "      0.0           17,911          3       5,970.3        5,876.0     3,824        8,211       2,195.0  pipe2                 \n",
            "      0.0           11,285          1      11,285.0       11,285.0    11,285       11,285           0.0  connect               \n",
            "      0.0            7,172          2       3,586.0        3,586.0     2,242        4,930       1,900.7  pthread_cond_broadcast\n",
            "      0.0            6,222          2       3,111.0        3,111.0     2,625        3,597         687.3  fwrite                \n",
            "      0.0            3,018          8         377.3          373.0       291          509          70.3  dup                   \n",
            "      0.0            1,614          1       1,614.0        1,614.0     1,614        1,614           0.0  bind                  \n",
            "      0.0              950          1         950.0          950.0       950          950           0.0  listen                \n",
            "\n",
            "[5/8] Executing 'cuda_api_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)     Med (ns)    Min (ns)    Max (ns)   StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  -----------  ---------  ----------  ------------  ----------------------\n",
            "     84.0       72,940,558          3  24,313,519.3    112,620.0     98,173  72,729,765  41,929,699.3  cudaMalloc            \n",
            "      8.1        7,003,955          1   7,003,955.0  7,003,955.0  7,003,955   7,003,955           0.0  cudaDeviceSynchronize \n",
            "      6.7        5,801,473          4   1,450,368.3  1,438,537.0    134,912   2,789,487   1,083,865.9  cudaMemcpy            \n",
            "      1.0          910,258          3     303,419.3    246,804.0    120,216     543,238     217,119.5  cudaFree              \n",
            "      0.2          161,703          1     161,703.0    161,703.0    161,703     161,703           0.0  cudaLaunchKernel      \n",
            "      0.0            1,573          1       1,573.0      1,573.0      1,573       1,573           0.0  cuModuleGetLoadingMode\n",
            "\n",
            "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                              Name                            \n",
            " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ------------------------------------------------------------\n",
            "    100.0        6,999,803          1  6,999,803.0  6,999,803.0  6,999,803  6,999,803          0.0  batchedMatMul(float *, float *, float *, int, int, int, int)\n",
            "\n",
            "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Count   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)      Operation     \n",
            " --------  ---------------  -----  -----------  -----------  ---------  ---------  -----------  ------------------\n",
            "     78.0        3,840,166      3  1,280,055.3  1,213,284.0     45,887  2,580,995  1,268,872.3  [CUDA memcpy HtoD]\n",
            "     22.0        1,080,231      1  1,080,231.0  1,080,231.0  1,080,231  1,080,231          0.0  [CUDA memcpy DtoH]\n",
            "\n",
            "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
            "\n",
            " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n",
            " ----------  -----  --------  --------  --------  --------  -----------  ------------------\n",
            "     19.399      3     6.466     6.291     0.524    12.583        6.031  [CUDA memcpy HtoD]\n",
            "      6.291      1     6.291     6.291     6.291     6.291        0.000  [CUDA memcpy DtoH]\n",
            "\n",
            "Generated:\n",
            "    /home/cuda/report8.nsys-rep\n",
            "    /home/cuda/report8.sqlite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SECOND TRIAL:** Finalizing After the previous runs and profiling\n",
        "\n",
        "\n",
        "\n",
        "*   16×16 blocks\n",
        "\n"
      ],
      "metadata": {
        "id": "RpyBtiheW-in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "// this is the reference implementation\n",
        "// you can change this to your heart's contempt\n",
        "#define TILE 16\n",
        "\n",
        "__global__\n",
        "void batchedMatMul(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "    __shared__ float M_tile[TILE][TILE];\n",
        "    __shared__ float N_tile[TILE][TILE];\n",
        "\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int b = blockIdx.z;  // batch index\n",
        "\n",
        "    if (row < m && col < n && b < batch) {\n",
        "        float value = 0.0f;\n",
        "\n",
        "        for (int kk = 0; kk < k; kk += TILE) {\n",
        "\n",
        "            // Load tile from M (M is same for all batches)\n",
        "            if (row < m && (kk + threadIdx.x) < k)\n",
        "                M_tile[threadIdx.y][threadIdx.x] = M[row * k + kk + threadIdx.x];\n",
        "            else\n",
        "                M_tile[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "            // Load tile from N (N is batched)\n",
        "            if ((kk + threadIdx.y) < k && col < n)\n",
        "                N_tile[threadIdx.y][threadIdx.x] = N[b * (k * n) + (kk + threadIdx.y) * n + col];\n",
        "            else\n",
        "                N_tile[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "            __syncthreads();\n",
        "\n",
        "            // Compute partial dot\n",
        "            for (int i = 0; i < TILE; i++) {\n",
        "                value += M_tile[threadIdx.y][i] * N_tile[i][threadIdx.x];\n",
        "            }\n",
        "\n",
        "            __syncthreads(); // prepare next tile load\n",
        "        }\n",
        "\n",
        "        P[b * (m * n) + row * n + col] = value;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "\n",
        "  float* M = (float*)malloc(sizeM * sizeof(float));\n",
        "  float* N = (float*)malloc(sizeN * sizeof(float));\n",
        "  float* P = (float*)malloc(sizeP * sizeof(float));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything\n",
        "  float *M_d;\n",
        "  float *N_d;\n",
        "  float *P_d;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&M_d, sizeM * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(M_d, M, sizeM * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(P_d, P, sizeP * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "  dim3 blockSize(16, 16);\n",
        "  dim3 numBlocks((n + blockSize.x - 1) / blockSize.x, (m + blockSize.y - 1) / blockSize.y, batch);\n",
        "  batchedMatMul<<<numBlocks, blockSize>>>(M_d, N_d, P_d, m, k, n, batch);\n",
        "\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything, e.g. add some logging\n",
        "  gpuErrchk(cudaFree(M_d));\n",
        "  gpuErrchk(cudaFree(N_d));\n",
        "  gpuErrchk(cudaFree(P_d));\n",
        "\n",
        "  free(M);\n",
        "  free(N);\n",
        "  free(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "6Ys4rptyJ5EJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8c4fd6aa-87ee-416f-be0f-2f8a91cc2520"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing bmatmul.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Compile, Run, Profile**"
      ],
      "metadata": {
        "id": "t4eaFcePJ_8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "gXV_BmFYKM2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv bmatmul.cpp bmatmul.cu\n",
        "!nvcc -arch=sm_75 bmatmul.cu -o bmatmul"
      ],
      "metadata": {
        "id": "PKs3tgz6KDYD"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./bmatmul 256 512 384 16 119"
      ],
      "metadata": {
        "id": "TlgkX-SPZhqf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "78d78219-66fb-4ee4-e14c-2ebc21a6e3f9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running batched matmul with m=256, k=512, n=384, batch=16, seed=119\n",
            "Checking results on CPU...\n",
            "All results matched, success!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Profile:"
      ],
      "metadata": {
        "id": "SBn4aD0gKRq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile --stats=true ./bmatmul 256 512 384 16 119"
      ],
      "metadata": {
        "id": "VRLC6R5AKRTs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8101fda2-5c55-44e6-d0b5-cce9e6c912f9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running batched matmul with m=256, k=512, n=384, batch=16, seed=119\n",
            "Checking results on CPU...\n",
            "All results matched, success!Generating '/tmp/nsys-report-2d56.qdstrm'\n",
            "[1/8] [========================100%] report9.nsys-rep\n",
            "[2/8] [========================100%] report9.sqlite\n",
            "[3/8] Executing 'nvtx_sum' stats report\n",
            "SKIPPED: /home/cuda/report9.sqlite does not contain NV Tools Extension (NVTX) data.\n",
            "[4/8] Executing 'osrt_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)   Max (ns)    StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  -------------  --------  -----------  ------------  ----------------------\n",
            "     98.9    5,413,158,258         63  85,923,147.0  100,147,792.0    63,440  301,890,148  46,014,012.1  poll                  \n",
            "      1.0       57,382,529        543     105,676.8       15,064.0       386   22,781,002     986,201.6  ioctl                 \n",
            "      0.1        2,793,435         31      90,110.8       16,413.0     8,881    2,020,973     359,017.8  mmap64                \n",
            "      0.0          685,062          1     685,062.0      685,062.0   685,062      685,062           0.0  pthread_cond_wait     \n",
            "      0.0          487,410         10      48,741.0       50,703.0    27,392       70,404      14,991.7  sem_timedwait         \n",
            "      0.0          432,109         49       8,818.6        8,263.0     2,162       28,804       3,965.2  open64                \n",
            "      0.0          262,634         40       6,565.9        3,597.0     1,865       32,249       6,841.2  fopen                 \n",
            "      0.0          201,727         15      13,448.5        7,101.0     1,682       91,716      22,280.8  mmap                  \n",
            "      0.0          137,403          2      68,701.5       68,701.5    46,615       90,788      31,235.0  pthread_create        \n",
            "      0.0          106,606         12       8,883.8        5,983.5     3,683       38,348       9,430.3  write                 \n",
            "      0.0           54,638         33       1,655.7        1,186.0       793        6,575       1,285.7  fclose                \n",
            "      0.0           41,164         20       2,058.2           52.0        51       40,078       8,948.9  fgets                 \n",
            "      0.0           35,930         64         561.4          580.5       190        1,269         233.9  fcntl                 \n",
            "      0.0           33,268          6       5,544.7        4,977.5     1,900        9,722       3,107.0  open                  \n",
            "      0.0           32,291          5       6,458.2        6,551.0     4,504        8,361       1,381.4  munmap                \n",
            "      0.0           24,237         15       1,615.8        1,415.0       740        3,891         889.8  read                  \n",
            "      0.0           22,780          2      11,390.0       11,390.0     7,945       14,835       4,872.0  socket                \n",
            "      0.0           18,301          3       6,100.3        5,663.0     3,175        9,463       3,166.7  pipe2                 \n",
            "      0.0           14,999          1      14,999.0       14,999.0    14,999       14,999           0.0  connect               \n",
            "      0.0            8,653          2       4,326.5        4,326.5     1,979        6,674       3,319.9  pthread_cond_broadcast\n",
            "      0.0            5,994          2       2,997.0        2,997.0     2,779        3,215         308.3  fwrite                \n",
            "      0.0            3,051          8         381.4          372.0       302          585          93.5  dup                   \n",
            "      0.0            1,618          1       1,618.0        1,618.0     1,618        1,618           0.0  bind                  \n",
            "      0.0              909          1         909.0          909.0       909          909           0.0  listen                \n",
            "\n",
            "[5/8] Executing 'cuda_api_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)     Med (ns)    Min (ns)    Max (ns)   StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  -----------  ---------  ----------  ------------  ----------------------\n",
            "     88.0       84,020,396          3  28,006,798.7    108,256.0    102,765  83,809,375  48,326,448.8  cudaMalloc            \n",
            "      6.3        6,008,389          4   1,502,097.3  1,479,123.5    136,825   2,913,317   1,133,910.4  cudaMemcpy            \n",
            "      4.5        4,334,703          1   4,334,703.0  4,334,703.0  4,334,703   4,334,703           0.0  cudaDeviceSynchronize \n",
            "      1.1        1,008,471          3     336,157.0    300,436.0    132,072     575,963     224,091.0  cudaFree              \n",
            "      0.1          137,707          1     137,707.0    137,707.0    137,707     137,707           0.0  cudaLaunchKernel      \n",
            "      0.0            1,944          1       1,944.0      1,944.0      1,944       1,944           0.0  cuModuleGetLoadingMode\n",
            "\n",
            "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                              Name                            \n",
            " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ------------------------------------------------------------\n",
            "    100.0        4,330,106          1  4,330,106.0  4,330,106.0  4,330,106  4,330,106          0.0  batchedMatMul(float *, float *, float *, int, int, int, int)\n",
            "\n",
            "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Count   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)      Operation     \n",
            " --------  ---------------  -----  -----------  -----------  ---------  ---------  -----------  ------------------\n",
            "     77.9        3,949,923      3  1,316,641.0  1,213,892.0     48,126  2,687,905  1,322,885.6  [CUDA memcpy HtoD]\n",
            "     22.1        1,122,566      1  1,122,566.0  1,122,566.0  1,122,566  1,122,566          0.0  [CUDA memcpy DtoH]\n",
            "\n",
            "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
            "\n",
            " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n",
            " ----------  -----  --------  --------  --------  --------  -----------  ------------------\n",
            "     19.399      3     6.466     6.291     0.524    12.583        6.031  [CUDA memcpy HtoD]\n",
            "      6.291      1     6.291     6.291     6.291     6.291        0.000  [CUDA memcpy DtoH]\n",
            "\n",
            "Generated:\n",
            "    /home/cuda/report9.nsys-rep\n",
            "    /home/cuda/report9.sqlite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **THIRD TRIAL:** Tweaking the TILE Size.\n",
        "\n",
        "\n",
        "*   32 × 32 blocks\n",
        "\n"
      ],
      "metadata": {
        "id": "_drFXRZgau3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "// this is the reference implementation\n",
        "// you can change this to your heart's contempt\n",
        "#define TILE 32\n",
        "\n",
        "__global__\n",
        "void batchedMatMul(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "    __shared__ float M_tile[TILE][TILE];\n",
        "    __shared__ float N_tile[TILE][TILE];\n",
        "\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int b = blockIdx.z;  // batch index\n",
        "\n",
        "    if (row < m && col < n && b < batch) {\n",
        "        float value = 0.0f;\n",
        "\n",
        "        for (int kk = 0; kk < k; kk += TILE) {\n",
        "\n",
        "            // Load tile from M (M is same for all batches)\n",
        "            if (row < m && (kk + threadIdx.x) < k)\n",
        "                M_tile[threadIdx.y][threadIdx.x] = M[row * k + kk + threadIdx.x];\n",
        "            else\n",
        "                M_tile[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "            // Load tile from N (N is batched)\n",
        "            if ((kk + threadIdx.y) < k && col < n)\n",
        "                N_tile[threadIdx.y][threadIdx.x] = N[b * (k * n) + (kk + threadIdx.y) * n + col];\n",
        "            else\n",
        "                N_tile[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "            __syncthreads();\n",
        "\n",
        "            // Compute partial dot\n",
        "            for (int i = 0; i < TILE; i++) {\n",
        "                value += M_tile[threadIdx.y][i] * N_tile[i][threadIdx.x];\n",
        "            }\n",
        "\n",
        "            __syncthreads(); // prepare next tile load\n",
        "        }\n",
        "\n",
        "        P[b * (m * n) + row * n + col] = value;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "\n",
        "  float* M = (float*)malloc(sizeM * sizeof(float));\n",
        "  float* N = (float*)malloc(sizeN * sizeof(float));\n",
        "  float* P = (float*)malloc(sizeP * sizeof(float));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything\n",
        "  float *M_d;\n",
        "  float *N_d;\n",
        "  float *P_d;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&M_d, sizeM * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(M_d, M, sizeM * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(P_d, P, sizeP * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "  dim3 blockSize(32, 32);\n",
        "  dim3 numBlocks((n + blockSize.x - 1) / blockSize.x, (m + blockSize.y - 1) / blockSize.y, batch);\n",
        "  batchedMatMul<<<numBlocks, blockSize>>>(M_d, N_d, P_d, m, k, n, batch);\n",
        "\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything, e.g. add some logging\n",
        "  gpuErrchk(cudaFree(M_d));\n",
        "  gpuErrchk(cudaFree(N_d));\n",
        "  gpuErrchk(cudaFree(P_d));\n",
        "\n",
        "  free(M);\n",
        "  free(N);\n",
        "  free(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1Lt8At_Xasv1",
        "outputId": "a4e50deb-8d00-4627-94e5-76acabcfd31b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing bmatmul.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv bmatmul.cpp bmatmul.cu\n",
        "!nvcc -arch=sm_75 bmatmul.cu -o bmatmul"
      ],
      "metadata": {
        "id": "pnKX66q-bHxp"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./bmatmul 256 512 384 16 119"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8_thpd0RbIyy",
        "outputId": "00714bbe-178e-4c9e-f3da-c216ea1586a7"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running batched matmul with m=256, k=512, n=384, batch=16, seed=119\n",
            "Checking results on CPU...\n",
            "All results matched, success!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile --stats=true ./bmatmul 256 512 384 16 119"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "veqhY3rWbJiV",
        "outputId": "f2ae382d-efb4-438b-f801-ca340457195f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running batched matmul with m=256, k=512, n=384, batch=16, seed=119\n",
            "Checking results on CPU...\n",
            "All results matched, success!Generating '/tmp/nsys-report-f8ea.qdstrm'\n",
            "[1/8] [========================100%] report10.nsys-rep\n",
            "[2/8] [========================100%] report10.sqlite\n",
            "[3/8] Executing 'nvtx_sum' stats report\n",
            "SKIPPED: /home/cuda/report10.sqlite does not contain NV Tools Extension (NVTX) data.\n",
            "[4/8] Executing 'osrt_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)   Max (ns)    StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  -------------  --------  -----------  ------------  ----------------------\n",
            "     99.1    5,907,162,956         68  86,870,043.5  100,140,169.0     1,482  294,391,023  43,853,568.0  poll                  \n",
            "      0.8       48,906,164        543      90,066.6       12,671.0       419   18,075,441     784,775.2  ioctl                 \n",
            "      0.0        2,083,059         31      67,195.5       10,584.0     8,268    1,340,061     237,398.8  mmap64                \n",
            "      0.0          874,647          1     874,647.0      874,647.0   874,647      874,647           0.0  pthread_cond_wait     \n",
            "      0.0          541,743         10      54,174.3       62,298.5    12,828       89,324      30,099.0  sem_timedwait         \n",
            "      0.0          414,411         49       8,457.4        7,670.0     2,485       21,667       3,755.5  open64                \n",
            "      0.0          248,685         40       6,217.1        3,444.0     1,386       31,924       7,169.2  fopen                 \n",
            "      0.0          181,985         15      12,132.3        7,932.0     1,643       70,307      16,893.8  mmap                  \n",
            "      0.0          127,721          2      63,860.5       63,860.5    45,011       82,710      26,657.2  pthread_create        \n",
            "      0.0           75,852         12       6,321.0        6,138.0     1,861       20,612       5,114.3  write                 \n",
            "      0.0           63,857         33       1,935.1        1,189.0       783       11,733       2,220.6  fclose                \n",
            "      0.0           42,991          6       7,165.2        6,407.0     2,000       15,662       4,763.0  open                  \n",
            "      0.0           39,939         20       1,997.0           52.0        51       38,824       8,668.2  fgets                 \n",
            "      0.0           34,710          6       5,785.0        5,819.5     3,745        7,845       1,368.4  munmap                \n",
            "      0.0           33,911         64         529.9          532.5       176        1,222         199.3  fcntl                 \n",
            "      0.0           23,902         15       1,593.5        1,501.0       496        3,396         859.7  read                  \n",
            "      0.0           21,072          2      10,536.0       10,536.0     6,527       14,545       5,669.6  socket                \n",
            "      0.0           18,887          3       6,295.7        6,492.0     3,240        9,155       2,962.4  pipe2                 \n",
            "      0.0           10,540          2       5,270.0        5,270.0     2,036        8,504       4,573.6  pthread_cond_broadcast\n",
            "      0.0            9,963          1       9,963.0        9,963.0     9,963        9,963           0.0  connect               \n",
            "      0.0            6,687          2       3,343.5        3,343.5     3,206        3,481         194.5  fwrite                \n",
            "      0.0            3,734          8         466.8          458.0       369          593          84.3  dup                   \n",
            "      0.0            1,448          1       1,448.0        1,448.0     1,448        1,448           0.0  bind                  \n",
            "      0.0              932          1         932.0          932.0       932          932           0.0  listen                \n",
            "\n",
            "[5/8] Executing 'cuda_api_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)     Med (ns)    Min (ns)    Max (ns)   StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  -----------  ---------  ----------  ------------  ----------------------\n",
            "     87.1       73,698,044          3  24,566,014.7    106,439.0     85,421  73,506,184  42,383,431.2  cudaMalloc            \n",
            "      6.9        5,873,908          4   1,468,477.0  1,453,762.5    215,278   2,751,105   1,035,503.0  cudaMemcpy            \n",
            "      4.6        3,931,446          1   3,931,446.0  3,931,446.0  3,931,446   3,931,446           0.0  cudaDeviceSynchronize \n",
            "      1.1          938,695          3     312,898.3    277,655.0    119,092     541,948     213,619.7  cudaFree              \n",
            "      0.2          149,464          1     149,464.0    149,464.0    149,464     149,464           0.0  cudaLaunchKernel      \n",
            "      0.0            1,668          1       1,668.0      1,668.0      1,668       1,668           0.0  cuModuleGetLoadingMode\n",
            "\n",
            "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                              Name                            \n",
            " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ------------------------------------------------------------\n",
            "    100.0        3,927,748          1  3,927,748.0  3,927,748.0  3,927,748  3,927,748          0.0  batchedMatMul(float *, float *, float *, int, int, int, int)\n",
            "\n",
            "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Count   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)      Operation     \n",
            " --------  ---------------  -----  -----------  -----------  ---------  ---------  -----------  ------------------\n",
            "     78.1        3,828,165      3  1,276,055.0  1,214,755.0     46,783  2,566,627  1,261,039.9  [CUDA memcpy HtoD]\n",
            "     21.9        1,073,255      1  1,073,255.0  1,073,255.0  1,073,255  1,073,255          0.0  [CUDA memcpy DtoH]\n",
            "\n",
            "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
            "\n",
            " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n",
            " ----------  -----  --------  --------  --------  --------  -----------  ------------------\n",
            "     19.399      3     6.466     6.291     0.524    12.583        6.031  [CUDA memcpy HtoD]\n",
            "      6.291      1     6.291     6.291     6.291     6.291        0.000  [CUDA memcpy DtoH]\n",
            "\n",
            "Generated:\n",
            "    /home/cuda/report10.nsys-rep\n",
            "    /home/cuda/report10.sqlite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FOURTH TRIAL** : Adding __Restrict__ Qualifiers\n",
        "\n",
        "\n",
        "\n",
        "*   Adding restrict to pointer parameters to inform compiler there is no aliasing, allowing better memory optimization.\n",
        "\n"
      ],
      "metadata": {
        "id": "YPgKZsVTeCBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "// this is the reference implementation\n",
        "// you can change this to your heart's contempt\n",
        "#define TILE 32\n",
        "\n",
        "__global__\n",
        "void batchedMatMul(float* __restrict__ M, float* __restrict__ N, float* __restrict__ P,\n",
        "                   int m, int k, int n, int batch) {\n",
        "    __shared__ float M_tile[TILE][TILE];\n",
        "    __shared__ float N_tile[TILE][TILE];\n",
        "\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int b = blockIdx.z;  // batch index\n",
        "\n",
        "    if (row < m && col < n && b < batch) {\n",
        "        float value = 0.0f;\n",
        "\n",
        "        for (int kk = 0; kk < k; kk += TILE) {\n",
        "\n",
        "            // Load tile from M (M is same for all batches)\n",
        "            if (row < m && (kk + threadIdx.x) < k)\n",
        "                M_tile[threadIdx.y][threadIdx.x] = M[row * k + kk + threadIdx.x];\n",
        "            else\n",
        "                M_tile[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "            // Load tile from N (N is batched)\n",
        "            if ((kk + threadIdx.y) < k && col < n)\n",
        "                N_tile[threadIdx.y][threadIdx.x] = N[b * (k * n) + (kk + threadIdx.y) * n + col];\n",
        "            else\n",
        "                N_tile[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "            __syncthreads();\n",
        "\n",
        "            // Compute partial dot\n",
        "            for (int i = 0; i < TILE; i++) {\n",
        "                value += M_tile[threadIdx.y][i] * N_tile[i][threadIdx.x];\n",
        "            }\n",
        "\n",
        "            __syncthreads(); // prepare next tile load\n",
        "        }\n",
        "\n",
        "        P[b * (m * n) + row * n + col] = value;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "\n",
        "  float* M = (float*)malloc(sizeM * sizeof(float));\n",
        "  float* N = (float*)malloc(sizeN * sizeof(float));\n",
        "  float* P = (float*)malloc(sizeP * sizeof(float));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything\n",
        "  float *M_d;\n",
        "  float *N_d;\n",
        "  float *P_d;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&M_d, sizeM * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(M_d, M, sizeM * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(P_d, P, sizeP * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "  dim3 blockSize(32, 32);\n",
        "  dim3 numBlocks((n + blockSize.x - 1) / blockSize.x, (m + blockSize.y - 1) / blockSize.y, batch);\n",
        "  batchedMatMul<<<numBlocks, blockSize>>>(M_d, N_d, P_d, m, k, n, batch);\n",
        "\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything, e.g. add some logging\n",
        "  gpuErrchk(cudaFree(M_d));\n",
        "  gpuErrchk(cudaFree(N_d));\n",
        "  gpuErrchk(cudaFree(P_d));\n",
        "\n",
        "  free(M);\n",
        "  free(N);\n",
        "  free(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MgVn_9eNeWZ4",
        "outputId": "11a3d28d-c619-4ebd-b38b-cd1e3a7267a2"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing bmatmul.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv bmatmul.cpp bmatmul.cu\n",
        "!nvcc -arch=sm_75 bmatmul.cu -o bmatmul"
      ],
      "metadata": {
        "id": "KsuETEnUenT8"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./bmatmul 256 512 384 16 119"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wObjhFW5eoOE",
        "outputId": "a16eb0b6-65a4-45e2-cdb8-2f53f55b3bd2"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running batched matmul with m=256, k=512, n=384, batch=16, seed=119\n",
            "Checking results on CPU...\n",
            "All results matched, success!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile --stats=true ./bmatmul 256 512 384 16 119"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mjNrOl3fepWw",
        "outputId": "d8c4c78e-e05b-43de-d763-998e355eba62"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running batched matmul with m=256, k=512, n=384, batch=16, seed=119\n",
            "Checking results on CPU...\n",
            "All results matched, success!Generating '/tmp/nsys-report-d6ca.qdstrm'\n",
            "[1/8] [========================100%] report11.nsys-rep\n",
            "[2/8] [========================100%] report11.sqlite\n",
            "[3/8] Executing 'nvtx_sum' stats report\n",
            "SKIPPED: /home/cuda/report11.sqlite does not contain NV Tools Extension (NVTX) data.\n",
            "[4/8] Executing 'osrt_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)   Max (ns)    StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  -------------  --------  -----------  ------------  ----------------------\n",
            "     99.1    5,915,020,571         68  86,985,596.6  100,143,800.5    48,662  298,558,429  44,075,100.6  poll                  \n",
            "      0.8       48,279,845        543      88,913.2       12,474.0       397   18,173,037     789,492.8  ioctl                 \n",
            "      0.0        2,056,197         31      66,328.9       12,601.0     7,921    1,322,858     234,286.9  mmap64                \n",
            "      0.0          744,621         10      74,462.1       63,811.5    15,237      220,092      58,493.0  sem_timedwait         \n",
            "      0.0          492,654          1     492,654.0      492,654.0   492,654      492,654           0.0  pthread_cond_wait     \n",
            "      0.0          402,321         49       8,210.6        7,291.0     2,007       20,254       3,378.8  open64                \n",
            "      0.0          273,074         40       6,826.9        3,497.0     1,453       42,812       9,309.8  fopen                 \n",
            "      0.0          240,884         15      16,058.9        8,489.0     2,111       90,313      22,162.3  mmap                  \n",
            "      0.0          120,808          2      60,404.0       60,404.0    52,052       68,756      11,811.5  pthread_create        \n",
            "      0.0           68,017         12       5,668.1        5,974.0     3,443        7,785       1,551.0  write                 \n",
            "      0.0           65,926         33       1,997.8        1,263.0       772       18,062       3,057.7  fclose                \n",
            "      0.0           38,917          5       7,783.4        6,420.0     5,697       12,605       2,877.4  munmap                \n",
            "      0.0           38,820          2      19,410.0       19,410.0     7,004       31,816      17,544.7  socket                \n",
            "      0.0           36,561         20       1,828.1           45.0        45       35,602       7,949.6  fgets                 \n",
            "      0.0           32,754         64         511.8          521.5       175        1,055         190.9  fcntl                 \n",
            "      0.0           31,400          3      10,466.7       10,257.0     2,934       18,209       7,639.7  pipe2                 \n",
            "      0.0           29,575          6       4,929.2        4,585.5     2,053        8,465       2,531.7  open                  \n",
            "      0.0           23,900         15       1,593.3        1,439.0       761        3,165         642.1  read                  \n",
            "      0.0           15,859          1      15,859.0       15,859.0    15,859       15,859           0.0  connect               \n",
            "      0.0            7,686          2       3,843.0        3,843.0     2,178        5,508       2,354.7  pthread_cond_broadcast\n",
            "      0.0            5,518          2       2,759.0        2,759.0     2,730        2,788          41.0  fwrite                \n",
            "      0.0            3,006          8         375.8          350.0       333          498          60.8  dup                   \n",
            "      0.0            1,479          1       1,479.0        1,479.0     1,479        1,479           0.0  bind                  \n",
            "      0.0              877          1         877.0          877.0       877          877           0.0  listen                \n",
            "\n",
            "[5/8] Executing 'cuda_api_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)     Med (ns)    Min (ns)    Max (ns)   StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  -----------  ---------  ----------  ------------  ----------------------\n",
            "     87.2       74,648,540          3  24,882,846.7    116,960.0     84,462  74,447,118  42,923,921.2  cudaMalloc            \n",
            "      7.0        5,973,551          4   1,493,387.8  1,475,351.5    177,580   2,845,268   1,089,736.5  cudaMemcpy            \n",
            "      4.6        3,933,571          1   3,933,571.0  3,933,571.0  3,933,571   3,933,571           0.0  cudaDeviceSynchronize \n",
            "      1.1          945,219          3     315,073.0    281,130.0    115,075     549,014     218,951.7  cudaFree              \n",
            "      0.2          150,179          1     150,179.0    150,179.0    150,179     150,179           0.0  cudaLaunchKernel      \n",
            "      0.0            1,398          1       1,398.0      1,398.0      1,398       1,398           0.0  cuModuleGetLoadingMode\n",
            "\n",
            "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                              Name                            \n",
            " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ------------------------------------------------------------\n",
            "    100.0        3,926,757          1  3,926,757.0  3,926,757.0  3,926,757  3,926,757          0.0  batchedMatMul(float *, float *, float *, int, int, int, int)\n",
            "\n",
            "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Count   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)      Operation     \n",
            " --------  ---------------  -----  -----------  -----------  ---------  ---------  -----------  ------------------\n",
            "     78.5        3,931,811      3  1,310,603.7  1,278,690.0     46,654  2,606,467  1,280,204.9  [CUDA memcpy HtoD]\n",
            "     21.5        1,078,886      1  1,078,886.0  1,078,886.0  1,078,886  1,078,886          0.0  [CUDA memcpy DtoH]\n",
            "\n",
            "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
            "\n",
            " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n",
            " ----------  -----  --------  --------  --------  --------  -----------  ------------------\n",
            "     19.399      3     6.466     6.291     0.524    12.583        6.031  [CUDA memcpy HtoD]\n",
            "      6.291      1     6.291     6.291     6.291     6.291        0.000  [CUDA memcpy DtoH]\n",
            "\n",
            "Generated:\n",
            "    /home/cuda/report11.nsys-rep\n",
            "    /home/cuda/report11.sqlite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FIFTH TRIAL : Shared Memory Padding\n",
        "\n",
        "* Padding shared memory to TILE+1 avoids shared memory bank conflicts"
      ],
      "metadata": {
        "id": "Y9is-3zkfEo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "// this is the reference implementation\n",
        "// you can change this to your heart's contempt\n",
        "#define TILE 32\n",
        "\n",
        "__global__\n",
        "void batchedMatMul(float* __restrict__ M, float* __restrict__ N, float* __restrict__ P,\n",
        "                   int m, int k, int n, int batch) {\n",
        "    __shared__ float M_tile[TILE][TILE + 1];\n",
        "    __shared__ float N_tile[TILE][TILE + 1];\n",
        "\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int b = blockIdx.z;  // batch index\n",
        "\n",
        "    if (row < m && col < n && b < batch) {\n",
        "        float value = 0.0f;\n",
        "\n",
        "        for (int kk = 0; kk < k; kk += TILE) {\n",
        "\n",
        "            // Load tile from M (M is same for all batches)\n",
        "            if (row < m && (kk + threadIdx.x) < k)\n",
        "                M_tile[threadIdx.y][threadIdx.x] = M[row * k + kk + threadIdx.x];\n",
        "            else\n",
        "                M_tile[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "            // Load tile from N (N is batched)\n",
        "            if ((kk + threadIdx.y) < k && col < n)\n",
        "                N_tile[threadIdx.y][threadIdx.x] = N[b * (k * n) + (kk + threadIdx.y) * n + col];\n",
        "            else\n",
        "                N_tile[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "            __syncthreads();\n",
        "\n",
        "            // Compute partial dot\n",
        "            for (int i = 0; i < TILE; i++) {\n",
        "                value += M_tile[threadIdx.y][i] * N_tile[i][threadIdx.x];\n",
        "            }\n",
        "\n",
        "            __syncthreads(); // prepare next tile load\n",
        "        }\n",
        "\n",
        "        P[b * (m * n) + row * n + col] = value;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "\n",
        "  float* M = (float*)malloc(sizeM * sizeof(float));\n",
        "  float* N = (float*)malloc(sizeN * sizeof(float));\n",
        "  float* P = (float*)malloc(sizeP * sizeof(float));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything\n",
        "  float *M_d;\n",
        "  float *N_d;\n",
        "  float *P_d;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&M_d, sizeM * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(M_d, M, sizeM * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(P_d, P, sizeP * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "  dim3 blockSize(32, 32);\n",
        "  dim3 numBlocks((n + blockSize.x - 1) / blockSize.x, (m + blockSize.y - 1) / blockSize.y, batch);\n",
        "  batchedMatMul<<<numBlocks, blockSize>>>(M_d, N_d, P_d, m, k, n, batch);\n",
        "\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything, e.g. add some logging\n",
        "  gpuErrchk(cudaFree(M_d));\n",
        "  gpuErrchk(cudaFree(N_d));\n",
        "  gpuErrchk(cudaFree(P_d));\n",
        "\n",
        "  free(M);\n",
        "  free(N);\n",
        "  free(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "s4QWcqQ9fNKC",
        "outputId": "8b90acc6-212d-4006-98af-dc7db8a61367"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing bmatmul.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv bmatmul.cpp bmatmul.cu\n",
        "!nvcc -arch=sm_75 bmatmul.cu -o bmatmul"
      ],
      "metadata": {
        "id": "-rlsalzYfX_0"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./bmatmul 256 512 384 16 119"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oyKBc5oufZar",
        "outputId": "3240e1d4-a6a8-4a58-f023-d029c403a007"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running batched matmul with m=256, k=512, n=384, batch=16, seed=119\n",
            "Checking results on CPU...\n",
            "All results matched, success!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile --stats=true ./bmatmul 256 512 384 16 119"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NI2Z6oscfcIS",
        "outputId": "bf3c89e2-11a1-4e53-845d-67ac0a2f2d2a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running batched matmul with m=256, k=512, n=384, batch=16, seed=119\n",
            "Checking results on CPU...\n",
            "All results matched, success!Generating '/tmp/nsys-report-c850.qdstrm'\n",
            "[1/8] [========================100%] report13.nsys-rep\n",
            "[2/8] [========================100%] report13.sqlite\n",
            "[3/8] Executing 'nvtx_sum' stats report\n",
            "SKIPPED: /home/cuda/report13.sqlite does not contain NV Tools Extension (NVTX) data.\n",
            "[4/8] Executing 'osrt_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)   Max (ns)    StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  -------------  --------  -----------  ------------  ----------------------\n",
            "     99.0    5,210,312,732         61  85,414,962.8  100,144,561.0     2,092  299,159,192  46,454,460.6  poll                  \n",
            "      0.9       49,878,875        543      91,858.0       12,872.0       395   18,200,859     790,681.9  ioctl                 \n",
            "      0.0        2,050,040         31      66,130.3       13,589.0     8,850    1,316,633     233,048.4  mmap64                \n",
            "      0.0          969,235         10      96,923.5       57,265.0    22,109      438,490     124,057.3  sem_timedwait         \n",
            "      0.0          481,765          1     481,765.0      481,765.0   481,765      481,765           0.0  pthread_cond_wait     \n",
            "      0.0          429,931         49       8,774.1        7,636.0     2,005       24,324       4,019.3  open64                \n",
            "      0.0          231,854         40       5,796.4        3,359.0     1,310       33,018       6,341.6  fopen                 \n",
            "      0.0          178,039         15      11,869.3        6,950.0     1,915       57,274      13,779.5  mmap                  \n",
            "      0.0          124,955          2      62,477.5       62,477.5    47,080       77,875      21,775.4  pthread_create        \n",
            "      0.0           61,156         12       5,096.3        5,528.0     1,358        8,119       2,049.0  write                 \n",
            "      0.0           53,891         33       1,633.1        1,059.0       711        6,251       1,322.3  fclose                \n",
            "      0.0           36,468         20       1,823.4           49.0        45       35,479       7,921.7  fgets                 \n",
            "      0.0           33,548         64         524.2          547.5       164        1,189         201.9  fcntl                 \n",
            "      0.0           32,900         15       2,193.3        1,476.0       805       11,649       2,667.4  read                  \n",
            "      0.0           32,613          5       6,522.6        6,493.0     4,681        8,602       1,583.2  munmap                \n",
            "      0.0           29,175          6       4,862.5        4,606.5     1,497        7,762       2,505.2  open                  \n",
            "      0.0           20,591          2      10,295.5       10,295.5     6,197       14,394       5,796.2  socket                \n",
            "      0.0           20,073          3       6,691.0        5,786.0     3,398       10,889       3,826.6  pipe2                 \n",
            "      0.0           10,955          1      10,955.0       10,955.0    10,955       10,955           0.0  connect               \n",
            "      0.0            7,613          2       3,806.5        3,806.5     2,164        5,449       2,322.8  pthread_cond_broadcast\n",
            "      0.0            5,693          2       2,846.5        2,846.5     2,401        3,292         630.0  fwrite                \n",
            "      0.0            3,394          8         424.3          393.5       281          606         105.5  dup                   \n",
            "      0.0            1,407          1       1,407.0        1,407.0     1,407        1,407           0.0  bind                  \n",
            "      0.0            1,103          1       1,103.0        1,103.0     1,103        1,103           0.0  listen                \n",
            "\n",
            "[5/8] Executing 'cuda_api_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)     Med (ns)    Min (ns)    Max (ns)   StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  -----------  ---------  ----------  ------------  ----------------------\n",
            "     86.4       77,062,602          3  25,687,534.0     77,668.0     71,631  76,913,303  44,362,817.4  cudaMalloc            \n",
            "      6.5        5,827,096          4   1,456,774.0  1,436,347.0    154,559   2,799,843   1,080,997.0  cudaMemcpy            \n",
            "      5.8        5,135,764          1   5,135,764.0  5,135,764.0  5,135,764   5,135,764           0.0  cudaDeviceSynchronize \n",
            "      1.1          996,660          3     332,220.0    290,957.0    128,323     577,380     227,354.4  cudaFree              \n",
            "      0.2          152,206          1     152,206.0    152,206.0    152,206     152,206           0.0  cudaLaunchKernel      \n",
            "      0.0            1,248          1       1,248.0      1,248.0      1,248       1,248           0.0  cuModuleGetLoadingMode\n",
            "\n",
            "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                              Name                            \n",
            " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ------------------------------------------------------------\n",
            "    100.0        5,132,873          1  5,132,873.0  5,132,873.0  5,132,873  5,132,873          0.0  batchedMatMul(float *, float *, float *, int, int, int, int)\n",
            "\n",
            "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Count   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)      Operation     \n",
            " --------  ---------------  -----  -----------  -----------  ---------  ---------  -----------  ------------------\n",
            "     79.3        3,911,525      3  1,303,841.7  1,240,963.0     49,055  2,621,507  1,287,378.2  [CUDA memcpy HtoD]\n",
            "     20.7        1,020,840      1  1,020,840.0  1,020,840.0  1,020,840  1,020,840          0.0  [CUDA memcpy DtoH]\n",
            "\n",
            "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
            "\n",
            " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n",
            " ----------  -----  --------  --------  --------  --------  -----------  ------------------\n",
            "     19.399      3     6.466     6.291     0.524    12.583        6.031  [CUDA memcpy HtoD]\n",
            "      6.291      1     6.291     6.291     6.291     6.291        0.000  [CUDA memcpy DtoH]\n",
            "\n",
            "Generated:\n",
            "    /home/cuda/report13.nsys-rep\n",
            "    /home/cuda/report13.sqlite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SIXTH TRIAL** : Preload M once per block and reuse across batches\n"
      ],
      "metadata": {
        "id": "ZyNrAjd-fzCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Didn't work out, lost all hope in it:("
      ],
      "metadata": {
        "id": "U5ncJJiagA8Z"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"report\"></a>\n",
        "## **Brief Report**"
      ],
      "metadata": {
        "id": "edxNOq-8PCdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You must fill in this section!!**\n",
        "\n",
        "Group information:\n",
        "- member-1: NAME, SURNAME, PERSON CODE\n",
        "- member-2: NAME, SURNAME, PERSON CODE\n",
        "- member-3: NAME, SURNAME, PERSON CODE\n",
        "- your group's NAME and LOGO<br><img src=\"https://static.wikia.nocookie.net/86-eighty-six/images/d/dc/Undertaker_emblem.png/revision/latest?cb=20210311091258\" alt=\"+0.005 points at the exam if you know the reference ^-^\" width=\"120\" border=\"0\">\n",
        "\n",
        "*Note: yes, groups can now have a logo - this is optional and merely for fun, if you don't feel like having one, no worries, in which case you may delete that itemize entry alongside this note :(*\n",
        "<!-- if you reeeeeally don't have ideas for a logo, before giving up, check this out: https://picrew.me/en/image_maker/47882 -->"
      ],
      "metadata": {
        "id": "5COx5mKMPF7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bullet points describing what you did with a short motivation (some - arguably stupid ? - examples are given):\n",
        "- used supershared L99 cache: this was the fastest way to raise the temperature and cook an egg on the GPU's heatsink\n",
        "- pinned DRAM chips to the wall and asked them to be faster: they did not comply\n",
        "- missread the assignment and implemented matrix diagonalization: now I have my own version of cuBLAS\n",
        "- relied so heavily on blockIdx.z that the results tried to escape the HBM stack: we politely asked them to stay\n",
        "- broke isolation and achieved priviledge escalation in Colab by kindly asking Google's chief sysadmin, we can now \"tweak\" the profiler's report: this was outside what was discussed during lectures, but social engineering is easier than writing good code\n",
        "\n",
        "*Note: possibly less than 8 entries of ~32 words each. More isn't necessarily better if nobody will read it.*\n",
        "\n",
        "*Note: the subject is \"the main things you came up with to improve the kernel\".*"
      ],
      "metadata": {
        "id": "PRKJGx2MR7LQ"
      }
    }
  ]
}